\chapter{Alberi decisionali}
\label{Capitolo 3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Vediamo come sfruttare una struttura dati discreta, l'\textbf{albero di
  decisione}, per affrontare problemi di \textit{concept learning}. Su questa
struttura implementeremo l'algoritmo chiamato \textbf{ID3} che tra le ipotesi
sceglie il risultato dell'apprendimento tramite esempi di addestramento. La
lista delle ipotesi in questo caso è enorme e la scelta è guidata dal cosiddetto
\textit{information gain}.
\section{Alberi di decisione come elementi esecutivi}
\begin{definizione}
Un albero di decisione prende in input un oggetto o una situazione descritta da un insieme di \textbf{attributi} e restituisce una \textit{"decisione"}, ovvero il valore predetto di uscita per tale input.
\end{definizione}
Gli attributi dell'input possono essere discreti o continui sebbene in questo corso ci limiteremo a studiare i casi discreti. L'apprendimento di una funzione a valori discreti viene chiamata \textbf{classificazione}, mentre nel caso continuo \textbf{regressione}. In particolare ci concentreremo sulla classificazione \textbf{booleana}, dove ogni esempio è classificato come vero o falso.\\  Un albero di decisione funziona eseguendo in sequenza una serie di test, dove \textbf{ogni nodo} interno all'albero corrisponde a un test sul valore di una delle proprietà, e le diramazioni uscenti dal nodo sono etichettate con tutti i possibili risultati. Ogni foglia specifica il valore da fornire in uscita. Lo scopo è quello di apprendere una definizione del \textbf{predicato obiettivo}. Se vogliamo esprimere un problema come un problema di apprendimento, per prima cosa dobbiamo definire gli attributi disponibili per descrivere gli esempi del dominio (proprio come abbiamo fatto durante il Concept Learning).\\
In figura \ref{dt} si ha un esempio dove si hanno attributi che hanno anche più di due valori. In rosso si hanno gli
attributi, in blu i valori degli attributi e in arancione le foglie coi
risultati. Le foglie sono le risposte booleane.
\section{Espressività degli alberi di decisione}
Ogni particolare ipotesi per un albero di decisione del \textbf{predicato obiettivo} preposto può essere vista come un'asserzione nella forma:
\[\forall s\,\,\, PredicatoObiettivo(s)\,\,\,\iff\,\,\,(P1(s)\vee P2(s)  \vee ... Pn(s))\]
Dove ogni condizione $P_i(s)$ è una congiunzione di test corrispondente a un cammino dalla radice dell'albero a un nodo foglia con esito positivo.
Possiamo quindi scegliere, al posto delle classiche funzioni booleane,
\textbf{alberi di decisione} per rappresentare un modello che applicato ad
esempi non visti ci dirà se applicare in output un'etichetta vera o falsa in
base a quanto appreso. Siamo in ambito di \textbf{apprendimento
  supervisionato}.Possiamo dire che gli alberi decisionali descrivono tutte le funzioni booleane (figura \ref{dt2} e figura \ref{dt3}). 
Notiamo a questo punto come l'albero decisionale in figura \ref{dt} è la
rappresentazione di:
\[(Outlook=Sunny\,\land\, Humidity=Normal)\]
\[\lor(Outlook=Overcast)\]
\[\lor(Outlook=Rain \land Wind=Weak) \] \\ Avendo $n$ funzioni booleane avremo un numero distinto di tabelle diverità (e quindi di alberi decisionali), ciascuna con $2^n$ righe, pari a $2^{2^{n}}$. In altre parole, gli alberi decisionali vanno bene per alcuni tipi di funzione e non per altri.\\
  \textbf{NOTA:} non esiste alcuna funzione booleana che non possa essere scritta sotto forma di un albero di decisione.\\
\begin{figure}
  \centering
  \begin{tikzpicture}[nodes={draw}, -, sibling distance=70pt, level
      distance=30pt]  
      \node{\color{red} outlook}
      child {node {\color{blue} sunny}
        child{node {\color{red} humidity}
          child{node {\color{blue} high}
            child{node {\color{orange} yes}}}
          child{node {\color{blue} normal}
            child{node {\color{orange} no}}}}}
      child {node {\color{blue} overcast}
        child {node {\color{orange} yes}}}
      child {node {\color{blue} rain}
        child{node {\color{red} wind}
          child{node {\color{blue} strong}
            child{node {\color{orange} no}}}
          child{node {\color{blue} weak}
            child{node {\color{orange} yes}}}}};
    \end{tikzpicture}
  \caption{Esempio di albero decisionale}
  \label{dt}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[nodes={draw}, -, sibling distance=70pt, level
    distance=30pt]  
    \node{\color{red} outlook}
    child {node {\color{blue} sunny}
      child{node {\color{red} wind}
        child{node {\color{blue} strong}
          child{node {\color{orange} no}}}
        child{node {\color{blue} weak}
          child{node {\color{orange} yes}}}}}
    child {node {\color{blue} overcast}
      child {node {\color{orange} no}}}
    child {node {\color{blue} rain}
      child{node {\color{orange} no}}};
  \end{tikzpicture}
  \caption{Esempio di albero decisionale per la formula $(Outlook=Sunny)\land
    (Wind=Weak)$} 
  \label{dt2}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[nodes={draw}, -, sibling distance=70pt, level
    distance=30pt]  
    \node{\color{red} outlook}
    child {node {\color{blue} overcast}
      child{node {\color{red} wind}
        child{node {\color{blue} strong}
          child{node {\color{orange} no}}}
        child{node {\color{blue} weak}
          child{node {\color{orange} yes}}}}}
    child {node {\color{blue} sunny}
      child{node {\color{orange} yes}}}
    child {node {\color{blue} rain}
      child{node {\color{red} wind}
        child{node {\color{blue} strong}
          child{node {\color{orange} no}}}
        child{node {\color{blue} weak}
          child{node {\color{orange} yes}}}}};
  \end{tikzpicture}
  \caption{Esempio di albero decisionale  per la formula $(Outlook=Sunny)\lor
    (Wind=Weak)$}
  \label{dt3}
\end{figure}
\section{Introduzione di alberi di decisione dagli esempi}
Un esempio per un albero di decisione booleano consiste in un vettore di attributi in input, $X$, e un singolo valore booleano in output $y$: ovvero un \textbf{Training Set} (come quello visto nel precedente capitolo).
Il problema di trovare un albero decisionale che si accorda con l'insieme di addestramento potrebbe sembrare difficile, ma in effetti esiste una soluzione banale: possiamo semplicemente costruire un albero che ha un cammino verso una foglia per ogni esempio, e lungo il cammino verificare ordinatamente il valore di ogni attributo ed eseguire il valore dato nell'esempio. Il problema di questo albero è che si limita a \textbf{memorizzare} le osservazioni senza estrarre dagli esempi alcuno schema, per cui non ci possiamo aspettare che sia capace di estrapolare soluzioni che non ha già incontrato. Per evitare queste problematiche ricorriamo all'algoritmo \textbf{ID3}: verificare per primo il valore dell'attributo più "significativo", ovvero quello che ha il maggiore impatto sulla classificazione. In questo modo cerchiamo di arrivare a una soluzione in un numero ridotto di test, ovvero: creare un albero più piccolo ma equivalente logicamente. In generale, dopo che il test del primo attriuto suddivide gli eempi, ognuno dei suoi esiti può essere considerato un problema di apprendimento completamente nuovo, con un attributo in meno e un insieme di esempi più piccolo. Possiamo perciò identificare quattro casi:
\begin{itemize}
    \item Se ci sono esempi sia positivi che negativi, scegliamo l'attributo che li suddivide meglio.
    \item Se tutti gli esempi rimanenti sono positivi \textbf{o} negativi, abbiamo fnito: possiamo rispondere Y o N rispettivamente agli esempi.
    \item Se non riesco a classificare alcun esempio, allora non è stata osservata alcuna situazione con quei valori di attributi. Restituiamo un valore di default.
    \item Se non rimane alcun attributo ma ci sono ancora esempi sia positivi che negativi, significa che tali esempi hanno esattamentela stessa descrizione (di valori sugli attributi) ma classificazione diverse. Questo può accadere quando alcuni dati sono scorretti, ovvero c'è \textbf{rumore} (vedremo in seguito come occuparcene).
\end{itemize}
\subsection{Algoritmo ID3}
Vista la difficoltà di scegliere l'albero si ha l'idea di scegliere un piccolo
albero di partenza (o più piccoli) e ricorsivamente l'attributo più
significativo (sia nei nodi rossi intermedi che nelle foglie) come radice per il
sotto-albero. Si fanno quindi crescere in modo 
coerente gli alberi piccoli scelti in partenza. Si punta ad arrivare a un
albero valido per tutti gli esempi ricevuti e anche per quelli non visti.\\
Iniziamo a vedere l'algoritmo anche se saranno necessarie molte specifiche:
\begin{algorithm}[H]
  \begin{algorithmic}
    \Function{ID3}{esempi, attrib, default}
    \If{esempi è vuoto return default}
    \EndIf
    \If{tutti gli \textit{esempi} hanno la stessa classificazione \textbf{return la classificazione}}
    \EndIf
    \If{\textit{attrib} è vuoto \textbf{return VALORE\_MAGGIORANZA(esempi)}}
    \EndIf
    \State $best \gets$ \textit{il ``miglior'' attributo di decisione per il
    prossimo nodo}
    \State $albero \gets$ \textit{un nuovo albero di decisione con alla radice \textbf{best}}
    \State $m \gets$ \textit{VALORE\_MAGGIORANZA(esempi)}
    \For {\textit{ogni valore Vi dell'attributo best}}
    \State $esempi \gets$ \textit{Elementi di esempi con $best = Vi$}
    \State $sottoAlb \gets$ \textit{ID3(esempi, attrib-best, m)}
    \State \textit{Aggiungi un ramo all'albero con etichetta Vi e sottoalbero \textbf{sottoalb}}
    \EndFor
    \State \textit{\textbf{return albero}}
    \EndFunction
  \end{algorithmic}
  \caption{Algoritmo ID3 (Iterative Dichotomiser 3)}
\end{algorithm}
Facciamo qualche osservazione finale sull'\textbf{algoritmo ID3}:
\begin{itemize}
  \item Lo spazio delle ipotesi è completo e sicuramente contiene il target.
  \item Ho in output una singola ipotesi.
  \item Non si ha backtracking sugli attributi selezionati, si procede con una
  ricerca greedy, trovando scelte buone localmente ma non ottime.
  \item Fa scelte basate su una ricerca statistica, facendo sparire incertezze
  sui dati.
  \item Il bias non è sulla classe iniziale, essendo lo spazio delle ipotesi
  completo, ma sulla scelta di solo alcune funzioni, preferendo alberi corti (e
  più semplici) e posizionando attributi ad alto information gain vicino alla
  radice. Il bias è quindi sulla preferenza di alcune ipotesi. Si usa il
  criterio euristico di \textit{rasoio di Occam}.
  \item $H$ è l'insieme potenza delle istanze $X$.
\end{itemize}
Sfortunatamento un algoritmo scritto in questo modo è destinato a compiere qualche errore, il nostro compito sarà quello di \textbf{scoprire quanto sara scorretto}.
\section{Scegliere L'insieme degli attributi}
Ricordiamo che l'idea è quella di minimizzare la profondità dell'albero risultante. L'idea è scegliere l'attributo che fornisce la classificazione più esatta possibile degli esempi.
Bisogna in primis capire cosa si intende come \textbf{attributo migliore}.
Gli esempi positivi sono gli esempi che già mi hanno restituito \textit{yes} mentre quelli negativi sono quelli che mi hanno restituito \textit{no}.
Vediamo quanto detto:
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[nodes={draw}, -, sibling distance=70pt, level
    distance=30pt]  
    \node{\color{red} $Attributo1$ \color{black} Esempi: [y1, y2, y3, y4, n1, n2, n3]}
    child {node {\color{blue} Esito1}
      child{node {\color{orange} [y1, y2, n1, n2]}}}
    child {node {\color{blue} Esito2}
      child{node {\color{orange} [y3, y4, n3]}}};
  \end{tikzpicture}
  \label{Albero1}
\end{figure}
La figura \ref{Albero1} nostra un albero che ha come attributo $Attributo1$ e come possibili esiti $Esito1$ e $Esito2$. Notiamo che questo attributo non è un buon attributo, poichè suddivide gli esempi in maniera \textit{mista}.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[nodes={draw}, -, sibling distance=70pt, level
    distance=30pt]  
    \node{\color{red} $Attributo2$ \color{black} Esempi: [y1, y2, y3, y4, n1, n2, n3]}
    child {node {\color{blue} Esito1}
      child{node {\color{orange} [y1, y2, y3, y4]}}}
    child {node {\color{blue} Esito2}
      child{node {\color{orange} [n1, n2, n3]}}};
  \end{tikzpicture}
  \label{Albero2}
\end{figure}
La figura \ref{Albero2} mostra invece un attributo che suddivide molto meglio gli esempi.
\begin{definizione}
  Un attributo perfetto suddivide gli esempi in insiemi tutti positivi o tutti negativi
\end{definizione}

Ora sia Mario Avolio (autore di questa sezione di appunti) e sia voi vi chiederete "\textit{Ma come faccio a capire quanto è buono un attributo?}. In teoria la misura dovrebbe avere valore massimo quando l'attributo è perfetto e minimo quando l'attributo non è utile per nulla(tipo Windows). Una misura adeguata è la quantità attesa di \textbf{informazione} fornita dall'attributo. Essa viene misurata in \textbf{bit}. \\ Un bit d'informazione è sufficiente a rispondere $Y$ o $N$ a una domanda di cui non si sa nulla, se le possibili risposte $v_i$ hanno probabilità $P(v_i)$ allora il \textbf{contenuto informativo} $I$ della risposta corretta è dato dalla definizione di \textbf{entropia}:
\begin{definizione}
  Dato un training set $S$ con valori $v_i,\,\, i=1\ldots n$. l'entropia di un
  insieme di bit misura la sua quantità d'informazione. In generale, se i possibili valori $v_i,\,\, i=1\ldots n$ hanno probabilità $P(v_i)$, il contenuto informatico $I$ della del training set $S$ è dato da:
  \[I(P(v_1),\ldots, P(v_n))=\sum_{i=1}-P(v_i)\log_2P(v_i)\]
  \end{definizione}
  \begin{definizione}
      Per stimare la probabilità delle possibili risposte, prima di verificare il valore degli attributi, si può guardare la proporzione di esempi negativi e positivi nell'insieme di addestramento. Supponiamo che contenga $p$ esempi positivi e $n$ esempi negativi: una stima dell'informazione contenuta nella risposta corretta è:
  \[I\left(\frac{p}{p+n},\frac{n}{p+n}\right)=-\frac{p}{p+n}\log_2\frac{p}{p+n}
    -\frac{n}{p+n}\log_2\frac{n}{p+n}\] 
      \end{definizione}
\begin{definizione}
    Se si decide che $p_+$ è la proporzione di esempi positivi e $p_-$ di
  quelli negativi (saranno quindi tra 0 e 1) possiamo misurare
  l'\textbf{impurità} di $S$ con l'entropia: 
  \[Entropy(S)=-p_+\log_2 p_+-p_-\log_2 p_-\]
  Avrò un'alta entropia se positivi e negativi sono ``metà e metà ''
  \end{definizione}

Il test di un singolo attributo non ci darà così tanta informazione, ma ce ne darà comunque un pò. Possiamo misurare esattamente tale quantità guardando quanta informazione ci è ancora necessaria \textit{dopo} aver effettuato il test. Ogni attributo divide l'insieme di addestramento in sottoinsiemi. Ogni sottoinsieme ha un certo numero di esempi positivi $p_i$ e un certo numero di esempi negativi $n_i$, per cui se seguiamo una certa diramazione avremo bisogno di un numero di bit pari a:
\[I\left(\frac{p_i}{p_i+n_i},\frac{n_i}{p_i+n_i}\right)\]
La probabilità che un esempio preso casualmente all'interno del Training Set abbia l'i-esimo valore per l'attributo considerato, è pari a:
\[\frac{p_i+n_i}{p+n}\]
Parliamo quindi di \textbf{information gain} $IG$ (in italiano: guadagno d'informazione) che viene calcolato su ogni
attributo $A$ e su $S$. In particolare il \textbf{guadagno d'informazione} ottenuto dal test dell'attributo è pari alla differenza tra il requisito informativo originale e quello corrente:
\[IG(S, A)=I\left(\frac{p}{p+n},\frac{n}{p+n}\right)-remainder(A)\]
dove:
\[remainder(A)=\sum_{i=1}^v \frac{p_i+n_i}{p+n}
  I\left(\frac{p_i}{p_i+n_i},\frac{n_i}{p_i+n_i}\right)\]
l'information gain è la riduzione aspettata nell'entropia per ordinare
$S$ sull'attributo $A$. Si sceglie l'attributo con il maggiore IG.\\
Possiamo riscrivere il conto come:
\[IG(S, A)=Entropy(S)-\sum_{v\in values(A)}\frac{|S_v|}{|S|}Entropy(S_v)\]
\begin{esempio}
  Vediamo l'esempio di calcolo di entropia di $A_1$ con [29+, 35-]:
  \[Entropy([29+, 35-])=
    -\frac{29}{64}\log_2\frac{29}{64}-\frac{35}{64}\log_2\frac{35}{64}=0.99\]
  Calcolo anche l'information gain di $A_1$, sapendo che
  $Entropy([21+, 5-])=0.71$ e $Entropy([8+, 30-])=0.74$,
  e quindi:
  \[IG(S, A_1)=0.99-\frac{26}{64}\cdot 0.71-\frac{38}{64}\cdot 0.74=0.27\]
  ugualmente calcolo $IG(S, A_2)=0.12$. \\
  Quindi so che devo scegliere $A_1$ in quanto $0.27 > 0.12$
\end{esempio}
\section{Valutare le prestazioni dell'algoritmo di apprendimento}
\begin{definizione}
  Un algoritmo di apprendimento è buono se produce ipotesi che riescono a predire con accuratezza la classificazione di esempi mai incontrati precedentemente.
\end{definizione}
Una predizione viene valutata in relazine a un \textbf{insieme di test}. Per questo motivo solitamente è doveroso adottare tale strategia:
\begin{enumerate}
    \item Raccogliere un grande insieme di test.
    \item Dividerlo in due insiemi disgiunti: uno per l'addestramento e uno per i test
    \item Applicare l'algoritmo di apprendimento all'insieme di addestramento, generando un'ipotesi $h$ (come già visto nel precedente capitolo sul Concept Learning)
    \item Misurare la percentuale di esempi nell'insieme di test che vengono classificati correttamente da $h$.
    \item Ripetere i passi 2 e 4 con dimensioni e insiemi diverse degli insiemi di addestramento.
\end{enumerate}
Il risultato di quanto descritto è un insieme di dati che può essere elaborato per fornire la qualità media della predizione in funzione delle dimensioni dell'insieme di addestramento. Questa funzione si può tracciare su un grafico, ottenendo la \textbf{curva di apprendimento}. \\ In questo contesto è opportuno evitare ciò che viene chiamato \textbf{peeking}: \textit{andare a creare un'ipotesi (NON generale) in base a un insieme di test che viene controllato a priori (e non a posteriori come si dovrebbe fare con dei test appunto).}
\section{Rumore e sovradattamento}
\subsection{Rumore}
Come abbiamo visto precedentemente, qualora due o più esempi contenessero la stessa descrizione ma avessero descrizioni differenti, l'algoritmo \textbf{IG3} non riuscirà a trovare un albero consistente per tutti gli esempi proposti. Una semplice soluzione per risolvere il problema è quella di far restituire a ogni nodo foglia la classificazione di maggioranza per il suo insieme di esempi. Questo solitamente porta a problemi poiché l'algoritmo genera un albero consistente per tutti gli esempi considerando quei attributi \textbf{irrilevanti} per operare distinzioni spurie tra gli esempi. 
\subsection{Overfitting}
Ogniqualvolta si opera con un insieme vasto d'ipotesi occorre stare attenti a non ricadere nell'\textbf{overfitting}(sovradattamento).
\begin{definizione}
  \textit{Definizione tratta da Wikipedia.}\\
  Definiamo formalmente l'\textbf{overfitting} come l'adattamento eccessivo,
  ovvero quando 
  un modello statistico molto complesso si adatta al campione perché ha un
  numero eccessivo di parametri rispetto al numero di osservazioni. SI ha
  quindi che un modello assurdo e sbagliato può adattarsi perfettamente se è
  abbastanza complesso rispetto alla quantità di dati disponibili.\\
  Nel machine learning se il learner viene addestrato troppo a lungo il modello
  potrebbe adattarsi a caratteristiche che sono specifiche solo del training
  set, ma che non hanno riscontro nel resto dei casi quindi le prestazioni sui
  dati non visionati saranno drasticamente peggiori.\\
  L'opposto è l'\textbf{underfitting}.
\end{definizione}

Se misuro l'errore di una ipotesi
$h$ sul training set ($error_{traini}(h)$) e poi misuro l'errore di quella
ipotesi sull'intero set delle possibili istanze
$D$ ($error_D(h)$) ho che l'ipotesi $h$ va in \textbf{overfit} sul quel data set
se:
\[error_{traini}(h) < error_{traini}(h') \,\,\land
  \,\, error_D(h)>error_D(h')\]
quindi se presa un'altra ipotesi questa è migliore della prima e ha un errore
sull'intera distribuzione delle ipotesi inferiore vado in \textit{overfit}. Il
problema è che non posso sapere se esiste tale $h'$. Per evitare il problema uso
sempre il rasoio di Occam scegliendo ipotesi semplici ed evitando di far
crescere l'albero quando lo ``split'' non è statisticamente significativo. Un
altro modo è quello di togliere pezzi, all'albero, che toccano poche istanze o
pure calcolare una \textit{misura di complessità dell'albero}, minimizzando la
grandezza dell'albero e gli errori del \textit{training set}, usando il
\textbf{Minimum Description Length (\textit{MDL})}.\\
In ID3 quindi posso scegliere sia in base all'\textit{information gain} massimo
o all'\textit{entropia} minima tra gli attributi.\\
Come specificato, una metodologia semplice da poter applicare in queste situazioni riguarda la \textbf{potatura dell'albero di decisione}. Essa opera impedendo la divisione ricorsiva su attributi che non sono chiaramente rilevamenti, anche quando i dati di un determinato nodo non sono classificati in modo uniforme. I lettori di questi appunti a questo punto si chiederanno sicuramente la seguente domanda: "Mario, se l'irrilevanza di un attributo si misura in relazione alla sua informazione fornita, quanto deve valere quest'ultima per etichettare un nodo come 'irrilevante'?". Per rispondere a questa domanda possiamo ricorrere a un \textbf{test di significatività} statistico: si comincia con un'\textbf{ipotesi nulla}, ovvero che l'attributo sia irrilevante; quindi i dati vengono analizzati per calcolare quanto si discostano effettivamente dalla totale assenza di pattern nei confronti degli esempi. Se il grado di derivazione risulta improbabile, questo viene considerato un buon indizio della presenza di un pattern significativo nei dati. \\ Un'altra possibile opzione da tenere in considerazione (magari insieme alla potatura) è la \textbf{validazione incrociata}: stimare quanto bene ogni ipotesi potrà predire dati mai incontrati.
\section{Esercizi Guidati}
Partiamo con un esercizio con find-S per coglierne le problematiche.
\begin{esercizio}
  Siano dati due attributi $A=\{1, 2, 3\}$ e $B=\{1, 2\}$. Diciamo che $H$ è un
  congiunzione di $and$ tra i valori degli attributi e delle istanze.\\
  Il concetto target è:
  \[c:=((A=1\lor A=2), B=1)\]
  Find-S può trovare $c$ in $H$?\\
  Prendo un training set contenente $\langle x_1=(1, 1), 1\rangle$ e
  $x_2=\langle(2, 1), 1\rangle$. Seguendo find-S 
  avremo la seguente traccia dell'algoritmo:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c}
      A & B\\
      \hline
      $\emptyset$ & $\emptyset$\\
      1 & 1 \\
      ? & 1
    \end{tabular}
  \end{table}
  Si arriva quindi a:
  \[S=\langle ?, 1\rangle\]
  Ma questa generalizzazione mi farebbe accettare $A=3$, cosa non prevista da
  $c$.\\
  Non posso quindi usare find-S in questo caso.
\end{esercizio}
Ricordiamo che un albero decisionale è formato da:
\begin{itemize}
  \item \textbf{Nodes (\textit{nodi})}: etichettati da i vari attributi.
  \item \textbf{Branches \textit{rami}}: etichettati con i possibili valori
  dell'attributo che etichetta il nodo sorgente del ramo.
  \item \textbf{Leef nodes (\textit{foglie})}: etichettati con gli outcome della
  previsione.
\end{itemize}
Un percorso dalla radice fino alla foglia mi rappresenta una congiunzione di
attributi mentre l'albero in se è una disgiunzione di congiunzioni, una
per ogni percorso radice-foglia. Un albero formalizza la congiunzione di
vincoli su attributi ma può estendere il linguaggio per accettare delle disgiunzioni di vincoli su questi ultimi.\\
Usando funzioni booleane \footnote{un attributo può avere valore $\top,\,\,\, T$ o
$\bot,\,\,\, F$} posso convertire tabelle di verità in alberi decisionali, avendo ogni percorso dalla radice a una foglia che rappresenta una
\textbf{regola}, e l'intero albero rappresenta la congiunzione di tutte le
regole. Le foglie quindi sono gli assegnamenti di verità della tabella.\\ 
Posso avere alberi diversi a seconda della scelta del nodo radice.\\
\begin{esempio}
  Vediamo i due alberi per la funzione booleana $\lor$.\\
  Abbiamo la tabella di verità:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
      $A$ & $B$ & $B\lor B$\\
      \hline
      $T$ & $T$ & $T$\\
      $T$ & $F$ & $T$\\
      $F$ & $T$ & $T$\\
      $F$ & $F$ & $F$
    \end{tabular}
  \end{table}
  rappresentata dai due alberi:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/dt1.pdf}
  \end{figure}
\end{esempio}
Essendo sempre nel concept learning, nel caso di funzioni booleane, anche il
target è booleano.
\begin{definizione}
  Con un albero decisionale posso rappresentare tutte le
  funzioni booleane
\end{definizione}
\begin{proof}
  Prendiamo una qualsiasi funzione booleana e la traduciamo in tabella di
  verità.\\
  A partire dalla tabella costruisco l'albero decisionale dove ogni percorso
  radice-foglia è un esempio, ovvero una riga, della tabella di verità.\\
  Dato che ogni funzione booleana può essere rappresentata con una tabella di
  verità posso costruire un albero decisionale per qualsiasi funzione booleana.
\end{proof}
Seppur il metodo appena descritto dimostri il definizione, non è sempre
efficiente, per via del fatto che necessita di memorizzare sempre tutto.
\begin{definizione}
  Avendo una funzione booleana con $n$ attributi allora posso costruire una
  tabella di verità con $2^n$ righe. Potenzialmente posso costruire 
  $2^{2^n}$ alberi decisionali differenti aventi lo stesso significato.
\end{definizione}
Vediamo quindi un algoritmo generale per la costruzione dell'albero:
\begin{enumerate}
  \item Si inizia con un albero vuoto.
  \item Scelgo un attributo opportuno da impostare come nodo radice per fare lo \textit{split} dei dati.
  \item Per ogni \textit{split} dell'albero:
  \begin{itemize}
    \item Se non ho altro da fare, calcolo la predizione con l'ultimo nodo foglia.
    \item Altrimenti si torna allo step due e si procede con un altro \textit{split}.
    \textit{split}
  \end{itemize}
\end{enumerate}
Bisognerà capire:
\begin{itemize}
  \item Come fare in modo ottimizzato lo `\textit{split}
  \item Quando fermare la costruzione dell'albero.
\end{itemize}
\begin{esempio}
  Prendiamo il solito esempio giocattolo di quando andare a fare sport in base
  al clima.\\
  In base al valore di \textit{outlook} ottengo tre tabelle, le quali si basano sul valore del campo \textit{overcast}, mantenendo comunque il target sul campo $play$ (nell'immagine il terzo arco è etichettato con
  $rainy$ e non con $text$):
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/dt2.pdf}
  \end{figure}
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c}
      outlook & temp & humidity & windy & play\\
      \hline
      overcast & H & H & $\bot$ & \color{darkgreen}{yes}\\
      overcast & C & N & $\top$ & \color{darkgreen}{yes}\\
      overcast & M & H & $\top$ & \color{darkgreen}{yes}\\
      overcast & H & N & $\bot$ & \color{darkgreen}{yes}
    \end{tabular}
    \caption{Tabella A}
  \end{table}
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c}
      outlook & temp & humidity & windy & play\\
      \hline
      sunny & H & H & $\bot$ & \color{red}{no}\\
      sunny & H & H & $\top$ & \color{red}{no}\\
      sunny & M & H & $\bot$ & \color{red}{no}\\
      sunny & C & N & $\bot$ & \color{darkgreen}{yes}\\
      sunny & M & N & $\top$ & \color{darkgreen}{yes}
    \end{tabular}
    \caption{Tabella B}
  \end{table}
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c}
      outlook & temp & humidity & windy & play\\
      \hline
      rainy & M & H & $\bot$ & \color{darkgreen}{yes}\\
      rainy & C & N & $\bot$ & \color{darkgreen}{yes}\\
      rainy & C & N & $\top$ & \color{red}{no}\\
      rainy & M & N & $\bot$ & \color{darkgreen}{yes}\\
      rainy & M & H & $\top$ & \color{red}{no}
    \end{tabular}
    \caption{Tabella C}
  \end{table}
 
  Vediamo un secondo i valori del target nei vari casi:
  \begin{itemize}
    \item Nel caso di \textit{overcast} ho quattro \textit{yes} e zero \textit{no}
    \item Nel caso di \textit{sunny} ho due \textit{yes} e tre \textit{no}
    \item Nel caso di \textit{rainy} ho tre \textit{yes} e due \textit{no}
  \end{itemize}
  Posso quindi usare una \textbf{funzione di costo} per fissare quando una
  distribuzione è omogenea (come nel caso di \textit{overcast}) e quando no (gli
  altri due casi). Lo \textit{split} infatti andrebbe fatto in base ai valori
  del target: più sono omogenei e meglio è; in quanto nel momento in cui si
  presenta un nuovo test per la classificazione con \textit{outlook} pari a
  \textit{overcast} saprò già cosa fare (in quanto nello storico delle
  esperienze ho sempre avuto \textit{yes}). Le altre due situazioni sono
  ambigue e, in quei due casi, devo procedere con la costruzione
  dell'albero per ottenere informazioni cercando di rimuovere l'incertezza.  
\end{esempio}
La \textbf{funzione costo} è alla base della strategia della costruzione
dell'albero.\\
Una strategia può essere quella ``a maggioranza'', prendendo l'attributo che con
i suoi valori ha meno disomogeneità. Per farlo calcolo la differenza tra $yes$ e
$no$ di ogni valore per un certo attributo, sommandone i risultati. Scelgo
l'attributo con più omogeneità (ovvero quello con la somma più bassa), che ha una
distribuzione più pulita dei risultati. Se ho valori con solo
$yes$ o solo $no$ sommo zero.
\begin{esempio}
  Prendendo le tre tabelle sopra, per \textit{outlook} avrei:
  \[0+1+1=2\]
  Se avessi avuto un attributo con:
  \begin{itemize}
    \item Primo valore: 1 \textit{yes} e 1 \textit{no}
    \item Secondo valore: 2 \textit{yes} e 0 \textit{no}
    \item Terzo valore: 0 \textit{yes} e 4 \textit{no}
    \item Quarto valore: 2 \textit{yes} e 4 \textit{no}
    \item Quinto valore: 2 \textit{yes} e 2 \textit{no} 
  \end{itemize}
  avrei avuto:
  \[1+0+0+2+2=5\]
\end{esempio}
Gli alberi di decisione possono essere utilizzati anche nel continuo, per
attributi numerici.
\begin{esempio}
  Prendiamo le istanze definite da due attributi, $x$ e $y$ e costruisco un
  albero di decisione che definisce in quali aree del piano ho $-$ e quali ho
  $+$.
  \newpage
  Si ha il seguente piano (\textbf{sull'asse delle $y$ il primo valore a partire
  dall'origine è un cinque non un sette}): 
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{img/dt3.pdf}
  \end{figure}
  E si ottiene, per esempio, il seguente albero decisionale:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/dt4.pdf}
  \end{figure}
  Dove a ciascun nodo è associata una condizione e agli archi il fatto che sia
  verificata o meno.\\
  Si nota come non si hanno tutte le condizioni, infatti, per esempio, con $x>3$
  mi basta $y>7$ per trovare il $+$ e $y<7$ per il $-$ (non dovendo andare
  specificatamente a guardare anche $y<5$ o $y>5$).\\
  \textbf{Quello disegnato è solo uno dei possibili alberi.}
\end{esempio}
\subsection{Esercizi su ID3}
Quando parliamo di \textit{entropia} stiamo volgendo un esperimento
concettuale. Prima dell'esperimento si ha una certa incertezza su un certo
eventi, che sparisce, con sorpresa, una volta eseguito l'esperimento se l'evento
accade. D'altro canto qualora accadesse un evento atteso siamo meno sorpresi del
fatto. Nel primo caso però si ritiene di ottenere molta informazione, nel
secondo caso poca. Come se ponessimo in una vasca quattro palline rosse, non
sarei stupito di estrarne una rossa, essendo quello che mi aspetto. D'altro
canto se ne aggiungo quattro verdi l'estrazione sarà inattesa e quindi più
interessante.\\ 
Si cerca un modo di quantificare questa ``sorpresa''. Riprendendo l'esempio
delle palline posso dire che nel primo caso (solo rosse) ho \textbf{bassa
  entropia} e nel secondo caso (palline miste) \textbf{alta entropia}. Un caso
intermedio sarebbe a \textbf{media entropia}.\\
Quindi, detta $P(x)$ la probabilità che avvenga un evento $x$ e con $I(p)$
l'informazione che ottengo dopo che l'evento si è verificato:
\begin{itemize}
  \item $P(x)=1\to I(p)=0$
  \item $P(x)=0\to I(p)=\infty$
\end{itemize}
L'informazione $I(p)$ e quindi una quantità non negativa:
\[I(p)\geq 0\]
Definiamo quindi l'informazione, detta anche \textit{self-information}, per un
evento $E$: 
\[I(E)=-\log_2(P(E))\]
Inoltre ho che è additiva se gli eventi sono indipendenti, ovvero:
\[I(p_1, p_2)=I(p_1)+I(p_2)\]
Bisogna però estendere l'informazione a tutte le possibili distribuzioni di
tutti i possibili esiti che ho in un esperimento concettuale.\\
Passiamo quindi alle definizioni matematiche per l'\textbf{entropia}:
\begin{itemize}
  \item $X\sim P_X$, presa una certa variabile $X$ dell'esperimento con una
  certa funzione di probabilità associata $P_X$
  \item $Val(X)=\{x_1,\ldots, x_n\}$, ovvero il range di valori della variabile
  $X$
  \item $p_i=P_X(x_i)$, probabilità per quel valore della variabile $X$
  \item $g:\mathbb{R}\to\mathbb{R}$ una funzione arbitraria tale per cui $g(X)$
  è una variabile causale. Si definisce l'aspettativa di $G(X)$ su $P$ come:
  \[E_P[g(X)]=\sum_{x\in Val(X)} g(x)\cdot P_X(x)\]
\end{itemize}
Giungendo quindi alla formula dell'\textbf{entropia} di una variabile:
\[H[X]=-\sum_{i=1}^n p_i\cdot\log_2 p_i=E_P[\log_2(p)]\]
Quindi:
\begin{definizione}
  L'\textbf{entropia}, definita da Claude Shannon (e quindi spesso chiamata
  \textbf{entropia di Shannon}), è l'informazione media associata ad
  una distribuzione di probabilità.
\end{definizione}
Vediamo un esempio:
\begin{esempio}
  Suppongo di lanciare una moneta, si ha:
  \[P(testa)=P(croce)=\frac{1}{2}\]
  Definiamo che testa è specificata da $X=0$ e croce da $X=1$.\\
  Calcoliamo quindi:
  \[H(p)=-p(0)\cdot \log_2 p(0)-p(1)\cdot\log_2
    p(1)=-2\cdot(\frac{1}{2}\cdot\log_2\frac{1}{2})=1\] 
  Una moneta ``onesta'' ha quindi entropia pari a 1 (avendo due probabili esiti
  equiprobabili non ho certezza del risultato).\\
  Ipotizziamo di avere una moneta magica che cade solo sulla testa (quindi
  $P(0)=1$ e $P(1)=0$:
  \[H(p)=-p(0)\cdot \log_2 p(0)-p(1)\cdot\log_2 p(1)=-\log_2 (1)=0\]
  Una moneta ``non onesta'' ha quindi entropia pari a 0 (ho infatti certezza del
  risultato).
\end{esempio}
\begin{esempio}
  Considero il seguente training set, con 4 esempi e target $T$:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
      example & A & B & C & D & T\\
      \hline
      $x_1$ & 0 & 0 & 1 & 1 & \color{darkgreen}{1}\\
      $x_2$ & 0 & 1 & 1 & 1 & \color{darkgreen}{1}\\
      $x_3$ & 0 & 1 & 0 & 0 & \color{red}{0}\\
      $x_4$ & 0 & 1 & 0 & 1 & \color{darkgreen}{1}\\
    \end{tabular}
  \end{table}
  Si ha quindi la seguente distribuzione di probabilità relativa al target
  (avendo un \textit{no} e tre \textit{yes}):
  \[P_T=\left[\frac{1}{4},\frac{3}{4}\right]\]
  e quindi posso calcolare l'entropia della tabella:
  \[H(P_T)=-P_T(0)\cdot \log_2 P_T(0)-P_T(1)\cdot\log_2
    P_T(1)=-\frac{1}{4}\cdot\log_2\frac{1}{4}-\frac{3}{4}\cdot\log_2
    \frac{3}{4}= 0.81\]
  La tabella può essere vista come un risultato di un esperimento concettuale.
\end{esempio}
\begin{definizione}
  Definiamo l'\textbf{entropia di una distribuzione condizionale}. Presa $X$
  come una variabile discreta arbitraria con valori $\{x_1,\ldots, x_n\}$, che
  hanno ciascuno probabilità $P_X(x_i)$ ho che, per la distribuzione
  condizionale:
  \[P{Y|X=x_i}(y_j)=P_{Y|X}(y_j|x_i)\]
  Ovvero la distribuzione dei valori della variabile $Y$ dato $X=x_i$.\\
  Quindi voglio sapere la probabilità di $Y$ condizionata da un esperimento
  precedente su $X$.\\
  Per l'entropia ho:
  \[H_{Y|X=x_i}=-\sum_{j=1}^m P_{Y|X}(y_j|x_i)\cdot \log_2 P_{Y|X}(y_j|x_i)\]
  quindi è la solita formula ma con la probabilità condizionale.
\end{definizione}
\begin{definizione}
  Definiamo \textbf{entropia condizionale} come il valore medio, ovvero il
  valore atteso, dell'entropia di $p_{Y|X=x_i}$ per ciascun valore di $X$ che
  condiziona $Y$, ovvero:
  \[H_{Y|X}=\sum_{i=1}^n P_X(x_i)H_{Y|X=x_i}\]
  ottenendo l'\textbf{entropia condizionale}:
  \[H[Y|X]=\sum P(x)\cdot H(Y|X=x)\]
\end{definizione}
\begin{esercizio}
  Considero il seguente training set, con 4 esempi e target $T$:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
      example & A & B & C & D & T\\
      \hline
      $x_1$ & 0 & 0 & 1 & 1 & \color{darkgreen}{1}\\
      $x_2$ & 0 & 1 & 1 & 1 & \color{darkgreen}{1}\\
      $x_3$ & 0 & 1 & 0 & 0 & \color{red}{0}\\
      $x_4$ & 0 & 1 & 0 & 1 & \color{darkgreen}{1}\\
    \end{tabular}
  \end{table}
  Abbiamo già calcolato l'entropia associabile al valore target $H[P_T]=0.81$.\\
  Passiamo ora all'uso d'iD3 per la costruzione dell'albero.\\
  Dobbiamo cercare gli split corretti tramite information gain, usando
  l'entropia condizionale.\\
  Partiamo con il primo attributo: $A$. Si ha che $P_A(0)=1$ e $P_A(1)=0$,
  quindi:
  \[H[T|A=0]=-p_{T|A}(0|0)\cdot \log_2(p_{T|A}(0|0))-p_{T|A}(1|0)\cdot
    \log_2(p_{T|A}(1|0))=\]
  \[-\frac{1}{4}\cdot\log_2\frac{1}{4}-
    \frac{3}{4}\cdot\log_2\frac{3}{4}=0.81\]
  (Risultato pari a quello dell'intero training set in quanto $A$ è sempre 0)\\
  Non devo calcolare $H[T|A=1]$ in quanto $A$ non è mai pari ad 1.\\
  Inoltre si ha:
  \[H[T|A]=P_A(0)\cdot H[T|A=0]+P_A(1)\cdot H[T|A=1]=1\cdot 0.81=0.81\]
  Posso quindi calcolare l'\textit{information gain}:
  \[IG[T;A]=H[T]-H[T|A]=0.81-0.81=0\]
  Quindi per $A$ ho la seguente distribuzione del target (avendo nel target 3
  esempi positivi e uno negativo e $A$ sempre con valore 0):
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/id1.pdf}
  \end{figure}
 \textbf{\textit{Ricordiamo che ID3 sceglie per lo splitting l'attributo che
     rende massimo l'information gain.}}\\
  Passo quindi all'attributo $B$. Si ha che $P_B(0)=\frac{1}{4}$ e
  $P_B(1)=\frac{3}{4}$, quindi:
  \[H[T|B=0]=-p_{T|B}(0|0)\cdot \log_2(p_{T|B}(0|0))-p_{T|B}(1|0)\cdot
    \log_2(p_{T|B}(1|0))=\]
  \[-0\cdot \log_2 0-1\cdot \log_2 1=0\]
  e:
  \[H[T|B=1]=-p_{T|B}(0|1)\cdot \log_2(p_{T|B}(0|1))-p_{T|B}(1|1)\cdot
    \log_2(p_{T|B}(1|1))=\]
  \[-\frac{1}{3}\cdot\log_2\frac{1}{3}-
    \frac{2}{3}\cdot\log_2\frac{2}{3}=0.91\]
  (ho quindi una partizione migliore con $B=0$)\\
  Inoltre si ha:
  \[H[T|B]=P_B(0)\cdot H[T|B=0]+P_B(1)\cdot H[T|B=1]=\frac{1}{4}\cdot
    0+\frac{3}{4}\cdot 0.91=0.68\]
  Posso quindi calcolare l'\textit{information gain}:
  \[IG[T;B]=H[T]-H[T|B]=0.81-0.68=0.13\]
  Quindi per $B$ ho:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/id2.pdf}
  \end{figure}
  Ho quindi un partizionamento più interessante.\\
  Passo quindi all'attributo $C$. Si ha che $P_C(0)=\frac{1}{2}$ e
  $P_C(1)=\frac{1}{2}$, quindi:
  \[H[T|C=0]=-p_{T|C}(0|0)\cdot \log_2(p_{T|C}(0|0))-p_{T|C}(1|0)\cdot
    \log_2(p_{T|C}(1|0))=\]
  \[-\frac{1}{2}\cdot \log_2 \frac{1}{2}-\frac{1}{2}\cdot \log_2 \frac{1}{2}=1\]
  e:
  \[H[T|C=1]=-p_{T|C}(0|1)\cdot \log_2(p_{T|C}(0|1))-p_{T|C}(1|1)\cdot
    \log_2(p_{T|C}(1|1))=\]
  \[-0\cdot \log_2 0-1\cdot \log_2 1=0\]
  Inoltre si ha:
  \[H[T|C]=P_C(0)\cdot H[T|C=0]+P_C(1)\cdot H[T|C=1]=\frac{1}{2}\cdot
    1+\frac{1}{2}\cdot 0=\frac{1}{2}\]
  Posso quindi calcolare l'\textit{information gain}:
  \[IG[T;C]=H[T]-H[T|C]=0.81-\frac{1}{2}=0.31\]
  Quindi per $C$ ho:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/id3.pdf}
  \end{figure}
  $C$ migliora ancora il partizionamento.\\
  Passo quindi all'attributo $D$. Si ha che $P_D(0)=\frac{1}{4}$ e
  $P_D(1)=\frac{3}{4}$, quindi:
  \[H[T|D=0]=-p_{T|D}(0|0)\cdot \log_2(p_{T|D}(0|0))-p_{T|D}(1|0)\cdot
    \log_2(p_{T|D}(1|0))=\]
  \[-1\cdot \log_2 1-0\cdot \log_2 0=0\]
  e:
  \[H[T|D=1]=-p_{T|D}(0|1)\cdot \log_2(p_{T|D}(0|1))-p_{T|D}(1|1)\cdot
    \log_2(p_{T|D}(1|1))=\]
  \[-0\cdot \log_2 0-1\cdot \log_2 1=0\]
  Inoltre si ha:
  \[H[T|D]=P_D(0)\cdot H[T|D=0]+P_D(1)\cdot H[T|D=1]=\frac{1}{4}\cdot
    0+\frac{3}{4}\cdot 0=0\]
  Posso quindi calcolare l'\textit{information gain}:
  \[IG[T;D]=H[T]-H[T|D]=0.81-0=0.81\]
  Quindi per $D$ ho:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/id4.pdf}
  \end{figure}
  $D$ rende quindi il massimo della ``purezza'' tra i valori di $D$ e quelli del
  target. Non ho incertezza nel partizionamento.\\
  Quindi, ricapitolando, ho i seguenti information gain:
  \begin{itemize}
    \item $IG[T;A]=0$
    \item $IG[T;B]=0.13$
    \item $IG[T;C]=0.31$
    \item $IG[T;D]=0.81$, \textbf{che è il valore massimo} e che rende minima la
    ``sorpresa''
  \end{itemize}
\end{esercizio}
\begin{esercizio}
  Considero il seguente training set, con 4 esempi e target $T$:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
      example & A & B & C & D & T\\
      \hline
      $x_1$ & 0 & 0 & 1 & $\ldots$ & \color{darkgreen}{1}\\
      $x_2$ & 0 & 1 & 1 & $\ldots$ & \color{darkgreen}{1}\\
      $x_3$ & 0 & 1 & 0 & $\ldots$ & \color{red}{0}\\
      $x_4$ & 0 & 1 & 0 & $\ldots$ & \color{darkgreen}{1}\\
    \end{tabular}
  \end{table}
  Bisogna riempire $D$ per rendere massimo l'information gain.
  \newpage
  Per farlo basta mettere gli stessi valori del target, in modo che sia
  l'attributo che meglio distribuisca i valori del target, ottenendo quindi lo
  stesso training set dell'esercizio precedente:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
      example & A & B & C & D & T\\
      \hline
      $x_1$ & 0 & 0 & 1 & 1 & \color{darkgreen}{1}\\
      $x_2$ & 0 & 1 & 1 & 1 & \color{darkgreen}{1}\\
      $x_3$ & 0 & 1 & 0 & 0 & \color{red}{0}\\
      $x_4$ & 0 & 1 & 0 & 1 & \color{darkgreen}{1}\\
    \end{tabular}
  \end{table}
  Un'alternativa è l'esatto opposto, in quanto otterrei la stessa
  ridistribuzione del target, rimuovendo ogni ``sorpresa'' ulteriore ma
  lasciando solo quella della tabella iniziale:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
      example & A & B & C & D & T\\
      \hline
      $x_1$ & 0 & 0 & 1 & 0 & \color{darkgreen}{1}\\
      $x_2$ & 0 & 1 & 1 & 0 & \color{darkgreen}{1}\\
      $x_3$ & 0 & 1 & 0 & 1 & \color{red}{0}\\
      $x_4$ & 0 & 1 & 0 & 0 & \color{darkgreen}{1}\\
    \end{tabular}
  \end{table}
\end{esercizio}
\begin{esercizio}
  Considero il seguente training set, con 4 esempi e target $T$:
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c}
      example & $f_1$ & $f_2$ & $f_3$ & T\\
      \hline
      $x_1$ & 1 & 1 & 1 & \color{darkgreen}{1}\\
      $x_2$ & 0 & 1 & 1 & \color{red}{0}\\
      $x_3$ & 0 & 0 & 1 & \color{darkgreen}{1}\\
      $x_4$ & 0 & 0 & 0 & \color{red}{0}\\
    \end{tabular}
  \end{table}
  Vogliamo completare il seguente albero decisionale:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.75]{img/id5.pdf}
  \end{figure}
  Partendo da $f_1$ so che se arriva un nuovo esempio non potrò proseguire da
  $[0-, 1+]$ in quanto so già che in quel caso avrei un'istanza positiva, valore
  minimo $0$.\\
  Quindi se $f_1=0$ vado a scegliere un nuovo attributo in quanto ancora non ho
  una distribuzione certa. Considero quindi le righe in cui $f_1=0$ e avanzo
  iterativamente studiando $f_2$ ed $f_3$. Studio quindi il sottoinsieme: 
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c}
      example  & $f_2$ & $f_3$ & T\\
      \hline
      $x_2$ & 1 & 1 & \color{red}{0}\\
      $x_3$ & 0 & 1 & \color{darkgreen}{1}\\
      $x_4$ & 0 & 0 & \color{red}{0}\\
    \end{tabular}
  \end{table}
  e avanzo fino a che non finisco gli attributi (o arrivo in un punto in cui,
  come per il ramo 1 di $f_1$ non posso più continuare).\\
  Per questo nuovo sottoinsieme calcolo:
  \[P_T=\left[\frac{2}{3},\frac{1}{4}\right]\]
  e quindi posso calcolare l'entropia della tabella:
  \[H(P_T)=-P_T(0)\cdot \log_2 P_T(0)-P_T(1)\cdot\log_2
    P_T(1)=-\frac{2}{3}\cdot\log_2\frac{2}{3}-\frac{1}{3}\cdot\log_2
    \frac{1}{3}= 0.91\]
  Passo quindi all'attributo $f_2$. Si ha che $P_{f_2}(0)=\frac{2}{3}$ e
  $P_{f_2}(1)=\frac{1}{3}$, quindi:
  \[H[T|f_2=0]=-p_{T|f_2}(0|0)\cdot \log_2(p_{T|f_2}(0|0))-p_{T|f_2}(1|0)\cdot
    \log_2(p_{T|f_2}(1|0))=\]
  \[-\frac{1}{2}\cdot \log_2 \frac{1}{2}-\frac{1}{2}\cdot \log_2 \frac{1}{2}=1\]
  e:
  \[H[T|f_2=1]=-p_{T|f_2}(0|1)\cdot \log_2(p_{T|f_2}(0|1))-p_{T|f_2}(1|1)\cdot
    \log_2(p_{T|f_2}(1|1))=\]
  \[-0\cdot \log_2 0-1\cdot \log_2 1=0\]
  Inoltre si ha:
  \[H[T|f_2]=P_{f_2}(0)\cdot H[T|f_2=0]+P_{f_2}(1)\cdot
    H[T|f_2=1]=\frac{2}{3}\cdot 1+\frac{1}{3}\cdot 0=\frac{2}{3}\]
  Posso quindi calcolare l'\textit{information gain}:
  \[IG[T;f_2]=H[T]-H[T|f_2]=0.81-\frac{2}{3}=0.25\]
  
  Passo quindi all'attributo $f_3$. Si ha che $P_{f_3}(0)=\frac{1}{3}$ e
  $P_{f_3}(1)=\frac{2}{3}$, quindi:
  \[H[T|f_3=0]=-p_{T|f_3}(0|1)\cdot \log_2(p_{T|f_3}(0|1))-p_{T|f_3}(1|1)\cdot
    \log_2(p_{T|f_3}(1|1))=\]
  \[-1\cdot \log_2 1-0\cdot \log_2 0=0\]
  e:
  \[H[T|f_3=1]=-p_{T|f_3}(0|0)\cdot \log_2(p_{T|f_3}(0|0))-p_{T|f_3}(1|0)\cdot
    \log_2(p_{T|f_3}(1|0))=\]
  \[-\frac{1}{2}\cdot \log_2 \frac{1}{2}-\frac{1}{2}\cdot \log_2 \frac{1}{2}=1\]
  Inoltre si ha:
  \[H[T|f_3]=P_{f_3}(0)\cdot H[T|f_3=0]+P_{f_3}(1)\cdot
    H[T|f_3=1]=\frac{1}{3}\cdot 0+\frac{2}{3}\cdot 1=\frac{2}{3}\]
  Posso quindi calcolare l'\textit{information gain}:
  \[IG[T;f_3]=H[T]-H[T|f_3]=0.81-\frac{2}{3}=0.25\]
  Avendo $f_1$ e $f_2$ lo stesso $IG$ prendo il primo e quindi ho:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/id6.pdf}
  \end{figure}
  Con $f_2$ che verrà attaccato al nodo $[2-, 1+]$ ``uscente'' da $f_1$.\\
  A questo punto, come sopra, abbiamo che il nodo $[1-, 0+]$ è foglia, avendo una
  distribuzione certa. Riduco quindi nuovamente il training set studiando solo
  gli esempi in cui $f_2$ vale 0, ovvero $x_3$ e $x_4$, per l'attributo $f_3$;
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
      example & $f_3$ & T\\
      \hline
      $x_3$ & 1 & \color{darkgreen}{1}\\
      $x_4$ & 0 & \color{red}{0}\\
    \end{tabular}
  \end{table}
  \newpage
  Non sono necessari conti in quanto $f_3$ è l'ultimo attributo rimasto ed è
  distribuito in questo modo:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.9]{img/id7.pdf}
  \end{figure}
  Avendo per di più entrambi i risultati con distribuzione certa.\\
  Il nodo di $f_3$ sarà attaccato al nodo $[1-, 1+]$ ``uscente'' da $f_2$,
  ottenendo così l'albero:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 1]{img/id8.pdf}
  \end{figure}
\end{esercizio}